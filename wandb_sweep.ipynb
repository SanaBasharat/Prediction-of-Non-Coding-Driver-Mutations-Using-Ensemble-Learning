{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.stats import ks_2samp\n",
    "import wandb\n",
    "from wandb.xgboost import WandbCallback\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"configuration.yaml\", \"r\") as yml_file:\n",
    "    config = yaml.load(yml_file, yaml.Loader)\n",
    "os.environ[\"WANDB_API_KEY\"] = config['WANDB_API_KEY']\n",
    "\n",
    "xgboost.set_config(verbosity=0)\n",
    "\n",
    "COLUMNS_TRAINING = config['COLUMNS_TRAINING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/final_dataset.csv')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "feat_df = pd.read_csv('data/feature_selected.csv')\n",
    "\n",
    "for col in data.columns[data.isna().any()].tolist():\n",
    "    data[col].fillna(0, inplace=True)\n",
    "\n",
    "data['TF_binding_site_agg'] = np.logical_or(data['TF_binding_site'], data['TF_binding_site_variant']).astype(int)\n",
    "\n",
    "data['TF_loss_add'] = data['TF_binding_site_agg'] + data['TF_loss']\n",
    "data['TF_gain_add'] = data['TF_binding_site_agg'] + data['TF_gain']\n",
    "data['TF_loss_diff_add'] = data['TF_binding_site_agg'] + data['TF_loss_diff']\n",
    "data['TF_gain_diff_add'] = data['TF_binding_site_agg'] + data['TF_gain_diff']\n",
    "\n",
    "data['SpliceAI_pred_DP_AG'] = abs(data['SpliceAI_pred_DP_AG'])\n",
    "data['SpliceAI_pred_DP_AL'] = abs(data['SpliceAI_pred_DP_AL'])\n",
    "data['SpliceAI_pred_DP_DG'] = abs(data['SpliceAI_pred_DP_DG'])\n",
    "data['SpliceAI_pred_DP_DL'] = abs(data['SpliceAI_pred_DP_DL'])\n",
    "\n",
    "\n",
    "data_test = data[(data['data_source'] == 'Rheinbay et al 2020') | (data['data_source'] == 'Dr.Nod 2023')]\n",
    "len_test_data = len(data_test)\n",
    "data_test = pd.concat([data_test, data[data['data_source'] == 'COSMIC'].sample(n=len_test_data)])   # get an equal amount of negative data\n",
    "data = data.drop(data_test.index, inplace=False).reset_index(drop=True, inplace=False)\n",
    "data_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "BIASED_COLUMNS = ['chr', 'ref_x', 'IG_C_gene', 'IG_D_gene', 'IG_J_gene', 'IG_J_pseudogene']\n",
    "\n",
    "COLUMNS_TRAINING = [x for x in COLUMNS_TRAINING if x not in BIASED_COLUMNS]\n",
    "COLUMNS_TRAINING = [x for x in COLUMNS_TRAINING if x in feat_df.columns]\n",
    "\n",
    "# COLUMNS_SHAP = [f'my_shap_{x}' for x in COLUMNS_TRAINING]\n",
    "\n",
    "for col in list(set(COLUMNS_TRAINING) - set(data.columns)):\n",
    "    data[col] = 0\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "data[COLUMNS_TRAINING] = min_max_scaler.fit_transform(data[COLUMNS_TRAINING])\n",
    "\n",
    "for col in list(set(COLUMNS_TRAINING) - set(data_test.columns)):\n",
    "    data_test[col] = 0\n",
    "\n",
    "data_test[COLUMNS_TRAINING] = min_max_scaler.fit_transform(data_test[COLUMNS_TRAINING])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 5vpl1ws9\n",
      "Sweep URL: https://wandb.ai/sanabasharat/thesis/sweeps/5vpl1ws9\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "  \"method\" : \"grid\",\n",
    "  \"parameters\" : {\n",
    "    \"learning_rate\" :{\n",
    "      \"values\": [0.0001, 0.001, 0.01]\n",
    "    },\n",
    "    # \"early_stopping_rounds\" :{\n",
    "    #   \"values\" : [1000, 2000, 4000]\n",
    "    # },\n",
    "    \"subsample\": {    # fraction of observations to be random samples for each tree\n",
    "      \"values\": [0.3, 0.5, 1.0]\n",
    "    },\n",
    "    # \"max_depth\": {\n",
    "    #   \"values\": [2, 4, 6]  \n",
    "    # }, # Used to control over-fitting as higher depth will allow the model to learn relations very specific to a particular sample\n",
    "    \"colsample_bytree\": {\n",
    "      \"values\": [0.5, 0.7, 1.0] # lower ratios avoid overfitting\n",
    "    },\n",
    "    \"gamma\": {\n",
    "      \"values\": [0, 3, 7] # larger values avoid overfitting\n",
    "    },\n",
    "    \"min_child_weight\": {\n",
    "      \"values\": [1, 5, 10] # larger values avoid overfitting\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='thesis', entity='sanabasharat')\n",
    "\n",
    "def train():\n",
    "  with wandb.init(job_type=\"sweep\") as run:\n",
    "    # for i in list_cvs: # for each of the 50 splits\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data[COLUMNS_TRAINING], data['driver'],\n",
    "                                                        random_state=104, \n",
    "                                                        test_size=0.20, \n",
    "                                                        shuffle=True)         # CODE SOURCE: containers_build\\boostdm\\training.py LIN 44\n",
    "    # x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=104) # 0.25 x 0.8 = 0.2\n",
    "    x_val = data_test[COLUMNS_TRAINING]\n",
    "    y_val = data_test['driver']\n",
    "    \n",
    "    bst_params = {\n",
    "        'objective': 'binary:logistic'\n",
    "        , 'base_score': y_train.mean()\n",
    "        , 'gamma': run.config['gamma'] #0\n",
    "        , 'learning_rate': run.config['learning_rate']\n",
    "        , 'max_depth': 4  # run.config['max_depth']\n",
    "        , 'n_estimators': 20000\n",
    "        , 'random_state': 42\n",
    "        , 'early_stopping_rounds': 2000 #run.config['early_stopping_rounds']\n",
    "        , 'eval_metric': 'logloss'\n",
    "        , 'subsample': run.config['subsample']\n",
    "        , 'reg_lambda': 1\n",
    "        , 'random_state': 42\n",
    "        , 'scale_pos_weight': 1\n",
    "        , 'silent': 1\n",
    "        , 'seed': 21\n",
    "        , 'reg_alpha': 0         # L1 regularization term on weight\n",
    "        , 'max_delta_step': 0    # positive value can help make the update step more conservative. generally not used\n",
    "        , 'min_child_weight': 1\n",
    "        , 'colsample_bylevel': 1.0\n",
    "        , 'colsample_bytree': run.config['colsample_bytree'] #1.0        # fraction of columns to be random samples for each tree\n",
    "        , 'booster': 'gbtree'\n",
    "        , 'n_jobs' : 1\n",
    "        , 'min_child_weight': run.config['min_child_weight'] #1\n",
    "    }\n",
    "    # params = XGB_PARAMS.copy()                                          \n",
    "    # params['n_estimators'] = 20000  # set it high enough to allow \"early stopping\" events below\n",
    "    # params['base_score'] = y_train.mean()\n",
    "    # params['n_jobs'] = 1\n",
    "    # params['seed'] = seed\n",
    "    model = XGBClassifier(**bst_params)\n",
    "\n",
    "    # train with xgboost\n",
    "    # learning_curve_dict = {}\n",
    "    model.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_test, y_test)],\n",
    "                        callbacks = [\n",
    "                            xgboost.callback.EvaluationMonitor(rank=0, period=1, show_stdv=False),\n",
    "                            WandbCallback()\n",
    "                        ],\n",
    "                        verbose = 0)\n",
    "\n",
    "    bst_params['n_estimators'] = model.best_iteration\n",
    "    model.set_params(**bst_params)\n",
    "    \n",
    "    bstr = model.get_booster()\n",
    "\n",
    "    # Log booster metrics\n",
    "    run.summary[\"best_ntree_limit\"] = bstr.best_ntree_limit\n",
    "    \n",
    "    # Get train and validation predictions\n",
    "    trnYpreds = model.predict_proba(x_train)[:,1]\n",
    "    valYpreds = model.predict_proba(x_val)[:,1] \n",
    "\n",
    "    # Log additional Train metrics\n",
    "    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_train, trnYpreds) \n",
    "    run.summary['train_ks_stat'] = max(true_positive_rate - false_positive_rate)\n",
    "    run.summary['train_auc'] = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "    run.summary['train_log_loss'] = -(y_train * np.log(trnYpreds) + (1-y_train) * np.log(1-trnYpreds)).sum() / len(y_train)\n",
    "\n",
    "    # Log additional Validation metrics\n",
    "    ks_stat, ks_pval = ks_2samp(valYpreds[y_val==1], valYpreds[y_val==0])\n",
    "    run.summary[\"val_ks_2samp\"] = ks_stat\n",
    "    run.summary[\"val_ks_pval\"] = ks_pval\n",
    "    run.summary[\"val_auc\"] = metrics.roc_auc_score(y_val, valYpreds)\n",
    "    run.summary[\"val_acc_0.5\"] = metrics.accuracy_score(y_val, np.where(valYpreds >= 0.5, 1, 0))\n",
    "    run.summary[\"val_log_loss\"] = -(y_val * np.log(valYpreds) + (1-y_val) * np.log(1-valYpreds)).sum() / len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 10 # number of runs to execute\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ensemble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
