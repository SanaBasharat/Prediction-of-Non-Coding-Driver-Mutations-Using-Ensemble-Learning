{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.stats import ks_2samp\n",
    "import wandb\n",
    "from wandb.xgboost import WandbCallback\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = config['WANDB_API_KEY']\n",
    "\n",
    "xgboost.set_config(verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"method\" : \"grid\",\n",
    "  \"parameters\" : {\n",
    "    \"learning_rate\" :{\n",
    "      \"values\": [0.001, 0.005, 0.01]\n",
    "    },\n",
    "    \"early_stopping_rounds\" :{\n",
    "      \"values\" : [1000, 2000, 4000]\n",
    "    },\n",
    "    \"subsample\": {    # fraction of observations to be random samples for each tree\n",
    "      \"values\": [0.5, 0.7, 0.8, 1.0]\n",
    "    },\n",
    "    \"max_depth\": {\n",
    "      \"values\": [4, 6]  \n",
    "    }, # Used to control over-fitting as higher depth will allow the model to learn relations very specific to a particular sample\n",
    "  }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='thesis', entity='sanabasharat')\n",
    "\n",
    "def train():\n",
    "  with wandb.init(job_type=\"sweep\") as run:\n",
    "    # for i in list_cvs: # for each of the 50 splits\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data[COLUMNS_TRAINING], data['driver'],\n",
    "                                                        random_state=104, \n",
    "                                                        test_size=0.25, \n",
    "                                                        shuffle=True)         # CODE SOURCE: containers_build\\boostdm\\training.py LIN 44\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=104) # 0.25 x 0.8 = 0.2\n",
    "    \n",
    "    bst_params = {\n",
    "        'objective': 'binary:logistic'\n",
    "        , 'base_score': y_train.mean()\n",
    "        , 'gamma': 0\n",
    "        , 'learning_rate': run.config['learning_rate']\n",
    "        , 'max_depth': 3\n",
    "        , 'n_estimators': 20000\n",
    "        , 'random_state': 42\n",
    "        , 'early_stopping_rounds': run.config['early_stopping_rounds']\n",
    "        , 'eval_metric': 'logloss'\n",
    "        , 'subsample': run.config['subsample']\n",
    "        , 'max_depth': run.config['max_depth']\n",
    "        , 'reg_lambda': 1\n",
    "        , 'random_state': 42\n",
    "        , 'scale_pos_weight': 1\n",
    "        , 'silent': 1\n",
    "        , 'seed': 21\n",
    "        , 'reg_alpha': 0         # L1 regularization term on weight\n",
    "        , 'max_delta_step': 0    # positive value can help make the update step more conservative. generally not used\n",
    "        , 'min_child_weight': 1\n",
    "        , 'colsample_bylevel': 1.0\n",
    "        , 'colsample_bytree': 1.0        # fraction of columns to be random samples for each tree\n",
    "        , 'booster': 'gbtree'\n",
    "        , 'n_jobs' : 1\n",
    "        , 'min_child_weight': 1\n",
    "    }\n",
    "    # params = XGB_PARAMS.copy()                                          \n",
    "    # params['n_estimators'] = 20000  # set it high enough to allow \"early stopping\" events below\n",
    "    # params['base_score'] = y_train.mean()\n",
    "    # params['n_jobs'] = 1\n",
    "    # params['seed'] = seed\n",
    "    model = XGBClassifier(**bst_params)\n",
    "\n",
    "    # train with xgboost\n",
    "    # learning_curve_dict = {}\n",
    "    model.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_test, y_test)],\n",
    "                        callbacks = [\n",
    "                            xgboost.callback.EvaluationMonitor(rank=0, period=1, show_stdv=False),\n",
    "                            WandbCallback()\n",
    "                        ],\n",
    "                        verbose = 0)\n",
    "\n",
    "    bst_params['n_estimators'] = model.best_iteration\n",
    "    model.set_params(**bst_params)\n",
    "    \n",
    "    bstr = model.get_booster()\n",
    "\n",
    "    # Log booster metrics\n",
    "    run.summary[\"best_ntree_limit\"] = bstr.best_ntree_limit\n",
    "    \n",
    "    # Get train and validation predictions\n",
    "    trnYpreds = model.predict_proba(x_train)[:,1]\n",
    "    valYpreds = model.predict_proba(x_val)[:,1] \n",
    "\n",
    "    # Log additional Train metrics\n",
    "    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_train, trnYpreds) \n",
    "    run.summary['train_ks_stat'] = max(true_positive_rate - false_positive_rate)\n",
    "    run.summary['train_auc'] = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "    run.summary['train_log_loss'] = -(y_train * np.log(trnYpreds) + (1-y_train) * np.log(1-trnYpreds)).sum() / len(y_train)\n",
    "\n",
    "    # Log additional Validation metrics\n",
    "    ks_stat, ks_pval = ks_2samp(valYpreds[y_val==1], valYpreds[y_val==0])\n",
    "    run.summary[\"val_ks_2samp\"] = ks_stat\n",
    "    run.summary[\"val_ks_pval\"] = ks_pval\n",
    "    run.summary[\"val_auc\"] = metrics.roc_auc_score(y_val, valYpreds)\n",
    "    run.summary[\"val_acc_0.5\"] = metrics.accuracy_score(y_val, np.where(valYpreds >= 0.5, 1, 0))\n",
    "    run.summary[\"val_log_loss\"] = -(y_val * np.log(valYpreds) + (1-y_val) * np.log(1-valYpreds)).sum() / len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 10 # number of runs to execute\n",
    "wandb.agent(sweep_id, function=train)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
