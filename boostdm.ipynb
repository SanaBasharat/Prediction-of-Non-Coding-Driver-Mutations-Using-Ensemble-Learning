{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "from xgboost import XGBClassifier\n",
    "with open(\"configuration.yaml\", \"r\") as yml_file:\n",
    "    config = yaml.load(yml_file, yaml.Loader)\n",
    "\n",
    "COLUMNS_TRAINING = config['COLUMNS_TRAINING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_filter(x_train, x_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Final preparation steps to achieve a competent CV data:\n",
    "    1) remove repeated data items in the test datasets\n",
    "       removing duplicate sites in test dataset --but not in training-- as repeated data in training provide us with\n",
    "       weight of evidence, whereas too many repeated data at testing can spoil our capacity to evaluate\n",
    "    2) random sampling to get a balanced test set\n",
    "    3) set training feature labels in a canonical order taken from configuration\n",
    "    \"\"\"\n",
    "\n",
    "    # reset index\n",
    "\n",
    "    x_test.reset_index(inplace=True, drop=True)\n",
    "    y_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # remove duplicates from test set\n",
    "\n",
    "    x_test['chr'] = x_test['chr'].astype(str)\n",
    "    x_test['start'] = x_test['start'].astype(int)\n",
    "    x_test = x_test.drop_duplicates(['start', 'alt'])\n",
    "    test_index = x_test.index\n",
    "    y_test = y_test.loc[y_test.index.intersection(test_index)]\n",
    "\n",
    "    # balance test set\n",
    "\n",
    "    total_index = set(x_test.index)\n",
    "    if y_test.mean() <= 0.5:\n",
    "        balance_index = y_test[y_test == 1].index.tolist()\n",
    "    else:\n",
    "        balance_index = y_test[y_test == 0].index.tolist()\n",
    "    remaining_index = list(set(total_index) - set(balance_index))\n",
    "    balance_index += list(np.random.choice(remaining_index, size=len(balance_index), replace=False))\n",
    "    x_test = x_test.loc[x_test.index.intersection(balance_index)]\n",
    "    y_test = y_test.loc[y_test.index.intersection(balance_index)]\n",
    "\n",
    "    # feature labels in standard order\n",
    "    avoid = ['chr', 'start', 'ref', 'alt']\n",
    "    features = list(filter(lambda x: x not in avoid, x_train))\n",
    "    # print(set(features))\n",
    "    # assert (set(features) == set(COLUMNS_TRAINING))\n",
    "    x_train = x_train[avoid + COLUMNS_TRAINING]\n",
    "    x_test = x_test[avoid + COLUMNS_TRAINING]\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def split_balanced(x_data, y_data, test_size=0.3):\n",
    "    \"\"\"Generate balanced train-test split\"\"\"\n",
    "\n",
    "    one_index  = list(y_data[y_data == 1].index)\n",
    "    zero_index = list(y_data[y_data == 0].index)\n",
    "\n",
    "    # randomly select n_ones indices from zero_index\n",
    "    zero_index = list(np.random.choice(zero_index, size=len(one_index), replace=False))\n",
    "\n",
    "    x_data_sub = x_data.loc[one_index + zero_index, :]\n",
    "    y_data_sub = y_data.loc[one_index + zero_index]\n",
    "\n",
    "    # the random state should be fixed prior to this call\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data_sub, y_data_sub, test_size=test_size)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def get_cv_sets_balanced(x_data, y_data, n, size):\n",
    "    \"\"\"Generate several balanced train-test sets\"\"\"\n",
    "\n",
    "    for _ in range(n):\n",
    "        x_train, x_test, y_train, y_test = split_balanced(x_data, y_data, test_size=size)\n",
    "        yield x_train, x_test, y_train, y_test\n",
    "\n",
    "def prepare(data, nsplits=10, test_size=0.2):\n",
    "\n",
    "    data.rename(columns={'response': 'label'}, inplace=True)\n",
    "\n",
    "    # keep 'pos' and 'chr' to run position-based filtering\n",
    "    avoid = ['cohort', 'gene', 'aachange', 'label', 'motif']  # include 'ref' 'alt'\n",
    "    features = list(filter(lambda x: x not in avoid, data.columns))\n",
    "\n",
    "    # regression datasets\n",
    "    x_data = data[features]\n",
    "    y_data = data['driver'] # label\n",
    "\n",
    "    # cv_list output has tuples (x_train, x_test, y_train, y_test) as elements\n",
    "    cv_list = get_cv_sets_balanced(x_data, y_data, nsplits, test_size)\n",
    "\n",
    "    # filter test data to prevent site repetitions\n",
    "    cv_list = [sort_filter(*arg) for arg in cv_list]\n",
    "\n",
    "    return cv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mutations(input_path):\n",
    "\n",
    "    data = pd.read_csv(input_path, sep=',', low_memory=False)\n",
    "    return data\n",
    "\n",
    "def generate(mutations, random_state=None, bootstrap_splits=50, cv_fraction=0.3):\n",
    "\n",
    "    # fix random seed (if provided)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    d_output = {}\n",
    "\n",
    "    # for gene, data in mutations.groupby('gene'):\n",
    "    cv_list = prepare(mutations.copy(), nsplits=bootstrap_splits, test_size=cv_fraction) # data.copy()\n",
    "    d_output = cv_list\n",
    "\n",
    "    return d_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations = load_mutations('data/final_dataset.csv')\n",
    "\n",
    "mutations.drop(['chr_y', 'start_y', 'end_y'], inplace=True, axis=1)\n",
    "mutations.rename({'chr_x': 'chr', 'start_x': 'start', 'end_x': 'end'}, inplace=True, axis=1)\n",
    "\n",
    "d_output = generate(mutations,\n",
    "                    random_state=104,\n",
    "                    bootstrap_splits=50,\n",
    "                    cv_fraction=0.3)\n",
    "\n",
    "with gzip.open('data/splits.pkl', 'wb') as f:\n",
    "    pickle.dump(d_output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3102"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_results = {\n",
    "        'models': [], 'split_number': [], 'x_test': [], 'y_test': [], 'learning_curves': []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(values):\n",
    "    \"\"\"Returns: optimal classification threshold and trained XGB model\"\"\"\n",
    "\n",
    "    x_train, x_test, y_train, y_test, split_number, seed = tuple(values)\n",
    "    XGB_PARAMS = config['XGB_PARAMS']\n",
    "    params = XGB_PARAMS.copy()\n",
    "    params['n_estimators'] = 20000  # set it high enough to allow \"early stopping\" events below\n",
    "    params['base_score'] = y_train.mean()\n",
    "    params['n_jobs'] = 1\n",
    "    params['seed'] = seed\n",
    "    myclassifier = XGBClassifier(**params)\n",
    "\n",
    "    # train with xgboost\n",
    "    learning_curve_dict = {}\n",
    "    myclassifier.train(x_train, y_train,\n",
    "                       eval_set=[(x_train, y_train), (x_test, y_test)],\n",
    "                       eval_metric='logloss',  # mcc_loss could be used here\n",
    "                       early_stopping_rounds=2000,\n",
    "                       callbacks=[\n",
    "                           xgboost.callback.record_evaluation(learning_curve_dict),\n",
    "                       ],\n",
    "                       verbose=False)\n",
    "\n",
    "    params['n_estimators'] = myclassifier.model.best_iteration\n",
    "    learning_curve_dict = {k: v['logloss'][:params['n_estimators']] for k, v in learning_curve_dict.items()}\n",
    "    myclassifier.model.set_params(**params)\n",
    "\n",
    "    return myclassifier, split_number, x_test, y_test, learning_curve_dict\n",
    "\n",
    "file_cv = 'data/splits.pkl'\n",
    "min_rows = 30   # Minimum number of rows to carry out training\n",
    "non_features = ['pos', 'chr', 'ref', 'alt']\n",
    "cores = 1\n",
    "with Pool(cores) as p:\n",
    "    with gzip.open(file_cv, 'rb') as f:\n",
    "        split_cv = pickle.load(f)\n",
    "\n",
    "    mean_size = np.nanmean([cv[0].shape[0] for cv in split_cv])\n",
    "    if mean_size < min_rows:\n",
    "        print(\"ERROR\")\n",
    "\n",
    "    list_cvs = []\n",
    "    for i, x in enumerate(split_cv):\n",
    "        x_list = list(x) + [i, np.random.randint(100000)]\n",
    "\n",
    "        # filter out non-features, i.e., columns not used for training\n",
    "        x_list[0] = x_list[0].drop(non_features, axis=1)\n",
    "        x_list[1] = x_list[1].drop(non_features, axis=1)\n",
    "        list_cvs.append(x_list)\n",
    "\n",
    "    for model, split_number, x_test, y_test, learning_curve in p.imap(\n",
    "            train, list_cvs):\n",
    "        dict_results['models'].append(model)\n",
    "        dict_results['split_number'].append(split_number)\n",
    "        dict_results['x_test'].append(x_test)\n",
    "        dict_results['y_test'].append(y_test)\n",
    "        dict_results['learning_curves'].append(learning_curve)\n",
    "\n",
    "    with gzip.open(output_file, 'wb') as f:\n",
    "    pickle.dump(dict_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ensemble",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
